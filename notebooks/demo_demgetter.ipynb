{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of DEM Download ##\n",
    "\n",
    "This notebook provides simple examples of how to search for, download, and merge geospatial data products from The National Map (TNM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..') #path to the script we need (the dem_getter directory)\n",
    "from dem_getter import dem_getter as dg #repository for functions to request/download/merge geospatial data from TNM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each function described below requires the user to input the name of the dataset they are interested in, and returns a list of paths where that data can be downloaded. There are several datasets available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available datasets are: {}'.format(list(dg.DATASETS_DICT.keys())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about the available datasets can be found at https://www.usgs.gov/3d-elevation-program/about-3dep-products-services. A map showing product coverage is available at https://apps.nationalmap.gov/lidar-explorer/#/."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Options ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several functions available depending on how the user wants to limit their search. The search options are:\n",
    "* **Bounding box [x_min, y_min, x_max, y_max]** \n",
    "    * Geographic longitude/latitude values expressed in decimal degrees in a comma-delimited list\n",
    "* **Polygon** \n",
    "    * A list of x,y coordinates describing a polygon\n",
    "* **Geodataframe** \n",
    "    * A dataframe containing a geometry column\n",
    "* **24k Quad Name**\n",
    "    * Name of a valid USGS 7.5'' quadrangle map. Information about available quad names can be found at:  \n",
    "     https://www.usgs.gov/faqs/where-can-i-find-indexes-usgs-topographic-maps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Defaults ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When inputting coordinates for a bounding box, the default setting... though the user may specify a different CRS\n",
    "The default coordinate system for the search option inputs is WGS84:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The default EPSG code for the input coordinate system is: \"+str(dg.EXPECTED_EPSG))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, each search is limited by a maximum number of products to be returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The maximum number of products returned is: \"+ str(dg.MAXITEMS))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some searches return multiple products within the same spatial extent; the default setting for the parameter do_exclude_redundant_data returns only the most current version of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Inputs ###\n",
    "\n",
    "Users can input a file name where the list of download paths can be saved, which the function will create if it doesn't already exist. By default, the code returns these paths as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDlPaths=os.path.join('..','test_data','test_dlPaths.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can also specify the type of data they want returned. Different resolutions of data have the following options available:  \n",
    "\n",
    "**STANDARD DEMS**  \n",
    "\n",
    "* 1 meter DEM - GeoTIFF, IMG                 \n",
    "    `Dataset code: 'DEM_1m'`\n",
    "* 5 meter DEM (Alaska only) - Varies         \n",
    "    `Dataset code: 'DEM_5m'`\n",
    "* NED 1/9 arc-second (3 m) - IMG             \n",
    "    `Dataset code: 'NED_1-9as'`\n",
    "* NED 1/3 arc-second (10 m) - GeoTIFF        \n",
    "    `Dataset code: 'NED_1-3as'`\n",
    "* NED 1 arc-second (30 m) – GeoTIFF         \n",
    "     `Dataset code: 'NED_1as'`\n",
    "* NED 2 arc-second (Alaska – 60 m) - GeoTIFF   \n",
    "     `Dataset code: 'NED_2as'`  \n",
    "\n",
    "**SOURCE DATA PRODUCTS**  \n",
    "  \n",
    "* Lidar Point Cloud (LPC) – LAS, LAZ         \n",
    "     `Dataset code: 'LPC'`  \n",
    "* Original Product Resolution (OPR) - Varies  \n",
    "     `Dataset code: 'OPR'` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Product Queries: Bounding Box ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all the DEM 1-meter products in GeoTIFF format within an arbitrary bounding box\n",
    "dl_list=dg.get_aws_paths(dataset='DEM_1m',xMin=-104,yMin=40,xMax=-101,yMax=41, filePath=saveDlPaths, dataType='GeoTIFF', doExcludeRedundantData=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user entered a path for the output to be saved, the generated file will look like this:\n",
    "\n",
    "<img src=../images/savedAwsPathsExample.PNG width='900'>\n",
    "\n",
    "The function will also return the fetched path as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all the NED 1/3 arc-second products in any format within Colorado\n",
    "dl_list=dg.get_aws_paths('NED_1-3as',-109,37,-102,41)\n",
    "print(dl_list[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the combination of dataset type and bounding box returns no products, the function will let the user know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search that returns no products\n",
    "dl_list=dg.get_aws_paths('NED_2as',-109,37,-102,41)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the function will halt if the user inputs an incorrect dataset or a datatype that doesn't go with a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for datasets not included in DATASETS_DICT raises a KeyError\n",
    "#Here we are just 'catching' the error and printing as a string to avoid showing the full traceback\n",
    "try:\n",
    "    dl_list=dg.get_aws_paths('bad_dataset',-109,37,-102,41)\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If searching for LPC products and datatype is specified, it must be LAS or LAZ, or LAS,LAZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dg.get_aws_paths('LPC',-121,35.8,-120.8,36, dataType='GeoTIFF')\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the same search with the correct data type specified.\n",
    "dg.get_aws_paths('LPC',-121,35.8,-120.8,36, dataType='LAS,LAZ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These warnings and errors also apply to the path retrieving functions described below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Product Queries: Polygon and Geodataframe ###\n",
    "\n",
    "These following examples demonstrate how to easily search for data within a polygon or shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd #use geopandas to read in shapefiles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load in a shapefile to use as an example-- Death Valley National Park\n",
    "dvnp = gpd.read_file(os.path.join('..','test_data','dvnp'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set_xlim(-1.32e7,-1.285e7)\n",
    "ax.set_ylim(4.2e6,4.57e6)\n",
    "dvnp.boundary.plot(ax=ax, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all the NED 1 arc-second products within Death Valley National Park\n",
    "#by default, the get_aws_paths_from_geodataframe function takes the geometry of the object in the first row of the geodataframe (row_idx=0). \n",
    "#Users can set row_idx to consider a different polygon within the geodataframe. \n",
    "dl_list=dg.get_aws_paths_from_geodataframe(dataset=\"NED_1as\",gdf=dvnp, rowIdx=0) \n",
    "\n",
    "print(\"Number of products found: \"+str(len(dl_list)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for products using a polygon to limit the results. The polygon must be expressed as a list of x,y coordinates; we can extract these from the polygon in the Death Valley geodataframe. Using the function get_aws_paths_polygon, we'll get the same results as above.\n",
    "\n",
    "If you didn't already have these polygons defined you could construct them as lists manually, for example a rectangle between 32 and 32.5 degrees north and 105 and 106 degrees west:\n",
    "\n",
    "    y = [32, 32.5, 32.5, 32, 32]\n",
    "    x = [-105, -105, -106, -106, -105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the polygon geometry\n",
    "geom=dvnp['geometry'][0]\n",
    "x=[]\n",
    "y=[]\n",
    "for idx in geom.exterior.coords:\n",
    "    x.append(idx[0])\n",
    "    y.append(idx[1])\n",
    "x[:5],y[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for all the DEM 1-meter products using the x,y coordinates\n",
    "dl_list=dg.get_aws_paths_polygon(\"NED_1as\",x,y,inputEPSG=3857)\n",
    "print(\"Number of products found: \"+str(len(dl_list)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Polygons ####\n",
    "\n",
    "When a geodataframe contains geometry in a multipolygon format, it must be converted to single polygons before using these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in multipolygon\n",
    "frank_church=gpd.read_file('../test_data/frank_church/').to_crs(3857) #Frank Church Wilderness in Idaho\n",
    "print(frank_church['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize\n",
    "ax=frank_church.boundary.plot(color='black',figsize=(10,10))\n",
    "ax.set_xlim(-1.295e7,-1.266e7)\n",
    "ax.set_ylim(5.45e6,5.82e6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find TNM products within Frank Church, first we have to convert the multipolygons to single polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frank_exploded=frank_church.explode(index_parts=True).reset_index()\n",
    "len(frank_exploded['geometry']) # the Frank Church wilderness is made up of 4 polygons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll get the paths to the products within these component polygons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pathlist=[]\n",
    "for i,row in frank_exploded.iterrows():\n",
    "    \n",
    "    paths = dg.get_aws_paths_from_geodataframe('NED_1as',frank_exploded,rowIdx=i)\n",
    "\n",
    "    if paths: #skips if polygon has no available products\n",
    "        for path in paths:\n",
    "            full_pathlist.append(path) #master list of file paths\n",
    "\n",
    "print(\"Number of products found: \"+str(len(full_pathlist)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Example Product Queries: 24k Quadrangle Names ###\n",
    "\n",
    "We can limit our product search using the name of a USGS 24k quadrangle map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quad map in Andrews, OR\n",
    "dg.get_aws_paths_from_24kQuadName(dataset='NED_1-3as', quadName='Andrews', stateName='Oregon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invalid quad names will return a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invalid quad name\n",
    "try:\n",
    "    dg.get_aws_paths_from_24kQuadName(dataset='NED_1-3as', quadName='NotaName', stateName='Notafornia')\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for paths within multiple quads and compile a list of all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test names to query-- adjacent quads in CO\n",
    "quad_names = ['Kinikinik','Rustic', 'Big Narrows','Poudre Park']\n",
    "\n",
    "dl_list=[]\n",
    "for name in quad_names:\n",
    "    paths= dg.get_aws_paths_from_24kQuadName(dataset='DEM_1m',quadName=name, stateName='Colorado')\n",
    "    \n",
    "    if paths: #skips if there are no available products\n",
    "        for path in paths:\n",
    "            dl_list.append(path) #master list of file paths\n",
    "\n",
    "#adjacent quads might return some of the same data products;\n",
    "# delete duplicates\n",
    "dl_list_unique=[*set(dl_list)]\n",
    "        \n",
    "print(\"Number of products returned: \"+str(len(dl_list)))\n",
    "print(\"Number of unique products: \"+str(len(dl_list_unique)))\n",
    "print(\"Preview of the first five products: \"+str(dl_list_unique[:5]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Merging Data ###\n",
    "\n",
    "The following section will demonstrate how to download and merge data, using a list of download paths that can be fetched with any of the functions described above.\n",
    "\n",
    "To use the download function, the user must input:\n",
    "* **List of download paths**\n",
    "* **Folder name to save the data to**\n",
    "    * If the input folder name does not exist, the function will create it\n",
    "  \n",
    "The batch_download function checks the size of the fetched data, and queries the user to continue with the download or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small bounding box outside Seward, Alaska with an Alaskan EPSG\n",
    "xmin = 523000\n",
    "xmax = 533223\n",
    "ymin = 666166\n",
    "ymax = 676800\n",
    "BBOX_EPSG = 6397 \n",
    "\n",
    "dl_list=dg.get_aws_paths('NED_2as',xmin,ymin,xmax,ymax,\n",
    "                         filePath = None,\n",
    "                         dataType = '',\n",
    "                         inputEPSG=BBOX_EPSG,\n",
    "                         doExcludeRedundantData=True)\n",
    "\n",
    "# #Dowload these files\n",
    "saved_paths = dg.batch_download(dl_list, os.path.join('..','test_data','test_downloads'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These downloaded products can now be merged together. The merge function requires:\n",
    "* **Input file list**\n",
    "    * A list of all the filenames to merge\n",
    "* **Output file path**\n",
    "    * New file name and path to save the data as\n",
    "\n",
    "Remember, you can always check the functions docstring for more information by running:\n",
    "\n",
    "    dg.merge_warp_dems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we'll merge together those downloaded files\n",
    "dg.merge_warp_dems(saved_paths,os.path.join('..','test_data','test_downloads','AK_merge.tif'),\n",
    "                   #Shown below are the defaults (e.g., what will be used if these arguments aren't specified)\n",
    "                   outExtent = [[xmin,xmax],[ymin,ymax]], #Output bounding box (if you want a crop of the merged product)\n",
    "                    outEPSG = BBOX_EPSG, #Output coordinate system EPSG reference, if none will use the input rasters\n",
    "                    pixelSize=None, #If you want to specify a specific pixel size\n",
    "                    doReturnGdalSourceResult = False, #If you want back a gdal Dataset object\n",
    "                    resampleAlg = 'cubic', #How to preform resambling when needed\n",
    "                    noDataValue = None, #Specify a different output no data value\n",
    "                    format = 'GTiff') #Specify a format to save"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below provides a quick way to fetch, download, and merge files from list of quads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of two quad names\n",
    "quad_names = ['Andrews', 'Juntura']\n",
    "state = 'Oregon'\n",
    "\n",
    "#download\n",
    "full_filelist=[]\n",
    "for name in quad_names:\n",
    "    paths= dg.get_aws_paths_from_24kQuadName('NED_1-3as',name, stateName=state)\n",
    "\n",
    "    if paths: #skips if polygon has no available products\n",
    "        filelist=dg.batch_download(paths,os.path.join('..','test_data','test_downloads')) #downloads files\n",
    "        for file in filelist:\n",
    "            full_filelist.append(file) #master list of file paths for merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and merge       \n",
    "mergeExtent = ([-119,-117],[41,44])\n",
    "EPSG=4326    \n",
    "dg.merge_warp_dems(full_filelist,os.path.join('..','test_data','test_downloads','quad_merge.tif'),mergeExtent,EPSG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll return to our multipolygon geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for multipolygons\n",
    "if 'MultiPolygon' in frank_church.geom_type.unique():\n",
    "    areasgdf = frank_church.explode(index_parts=True).reset_index()\n",
    "else:\n",
    "    areasgdf = frank_church.copy()\n",
    "\n",
    "#Next we loop through each entry of the dataframe and get the aws path and perform a batch download. \n",
    "#This loop adds a column for the filelist assoiated with that polygon.\n",
    "downloadPath = os.path.join('..','test_data','test_downloads')\n",
    "\n",
    "full_pathlist = []\n",
    "\n",
    "if not os.path.exists(downloadPath):\n",
    "    os.makedirs(downloadPath)\n",
    "\n",
    "areasgdf['filelists'] = None\n",
    "\n",
    "for i,row in areasgdf.iterrows():\n",
    "    paths = dg.get_aws_paths_from_geodataframe('NED_1as', areasgdf, rowIdx=i)\n",
    "\n",
    "    if paths: #skips if polygon has no available products\n",
    "        filelist = dg.batch_download(paths, downloadPath)\n",
    "        areasgdf.at[i,'filelists'] = filelist\n",
    "        for path in paths:\n",
    "            full_pathlist.append(path)\n",
    "print(\"Number of products found: \"+str(len(full_pathlist)))\n",
    "\n",
    "#Now we loop through the dataframe and merge all the rasters in the filelist for that entry. \n",
    "outPaths = []\n",
    "epsg = 3857\n",
    "\n",
    "for i, row in areasgdf.iterrows():\n",
    "    paths = areasgdf['filelists'][i]\n",
    "    if paths:\n",
    "        fname = f'polymerge_{i}.tif'\n",
    "        outPath = os.path.join(downloadPath, fname)\n",
    "        \n",
    "        x,y = areasgdf['geometry'][i].exterior.coords.xy\n",
    "        mergeExtent = ([min(x), max(x)],[min(y),max(y)])\n",
    "        \n",
    "        print(outPath)\n",
    "        dg.merge_warp_dems(paths, outPath, mergeExtent, epsg)\n",
    "        outPaths.append(outPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove paths to clear up space\n",
    "for i, row in areasgdf.iterrows():\n",
    "    paths = areasgdf['filelists'][i]\n",
    "    if paths:\n",
    "        for path in paths:\n",
    "            try:\n",
    "                os.remove(path)\n",
    "                print(f\"File deleted: {os.path.split(path)[-1]}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {os.path.split(path)[-1]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {os.path.split(path)[-1]}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2963f68853807b8ed2d6ba174196e13d966fabc9b2e87680f5adf13c54295805"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
